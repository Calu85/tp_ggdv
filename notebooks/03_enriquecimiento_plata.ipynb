{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0303ce6d",
   "metadata": {},
   "source": [
    "# Clase 5 – Enriquecimiento de la capa Plata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938a3d2d",
   "metadata": {},
   "source": [
    "En esta notebook se\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c87778",
   "metadata": {},
   "source": [
    "## Importar las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573fb540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Deshabilitar warnings futuros\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Ajustar el ancho máximo para impresión en consola\n",
    "pd.set_option('display.max_columns', None)  # Mostrar todas las columnas\n",
    "pd.set_option('display.width', 300)         # Ajustar a un ancho suficiente en consola\n",
    "pd.set_option('display.max_colwidth', None) # Evitar recortes en contenido de celdas\n",
    "\n",
    "print(\"Importación de librerías completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47201f0b",
   "metadata": {},
   "source": [
    "## Configuración de paths y carpetas del proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path('..').resolve()\n",
    "RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "BRONCE_DIR = BASE_DIR / 'data' / 'bronce'\n",
    "PLATA_DIR = Path(\"../data/plata\")\n",
    "\n",
    "archivo_plata = PLATA_DIR / \"misiones_plata.csv\"\n",
    "archivo_horario = PLATA_DIR / \"misiones_horario.csv\"\n",
    "\n",
    "print(\"Iniciación de carpetas del proyecto completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cba45b",
   "metadata": {},
   "source": [
    "## Carga del dataset y verificación de estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset diario\n",
    "try:\n",
    "    df_plata = pd.read_csv(archivo_plata, parse_dates=[\"FECHA\"])\n",
    "    print(\"Dataset diario cargado correctamente\")\n",
    "except FileNotFoundError:\n",
    "    print(\"El archivo diario no fue encontrado\")\n",
    "\n",
    "# Cargar el dataset horario\n",
    "try:\n",
    "    df_horario = pd.read_csv(archivo_horario, parse_dates=[\"FECHA_HORA\"])\n",
    "    print(\"Dataset horario cargado correctamente\")\n",
    "except FileNotFoundError:\n",
    "    print(\"El archivo horario no fue encontrado\")\n",
    "\n",
    "# Vista preliminar\n",
    "print(\"\\n Dataset diario:\")\n",
    "df_plata.info()\n",
    "print(\"\\n\")\n",
    "print(df_plata.head())\n",
    "\n",
    "print(\"\\n Dataset horario:\")\n",
    "df_horario.info()\n",
    "print(\"\\n\")\n",
    "print(df_horario.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2209b877-0cff-4dc7-ba04-d9ed38525d7a",
   "metadata": {},
   "source": [
    "Este paso permite validar la estructura general, tipos de datos y posibles columnas faltantes tanto en el dataset diario como en el horario. Si todo está correcto, avanzaremos con el enriquecimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a9de26",
   "metadata": {},
   "source": [
    "## Detección y análisis de fechas faltantes\n",
    "\n",
    "Una vez verificada la estructura del dataset diario, procedemos a identificar si existen fechas faltantes en la serie por estación. \n",
    "\n",
    "Esto nos permitirá decidir estrategias para tratar los días sin registros, como imputación o exclusión.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392108e6-7ec0-42ce-a42f-5f1c440573fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar el rango completo de fechas esperadas\n",
    "fechas_totales = pd.date_range(start=df_plata['FECHA'].min(), end=df_plata['FECHA'].max(), freq='D')\n",
    "\n",
    "# Obtener todas las combinaciones posibles de fecha y estación\n",
    "estaciones = df_plata['ESTACION'].unique()\n",
    "index_completo = pd.MultiIndex.from_product([fechas_totales, estaciones], names=['FECHA', 'ESTACION'])\n",
    "\n",
    "# Reindexar para insertar NaNs explícitos en las fechas faltantes\n",
    "df_plata = df_plata.set_index(['FECHA', 'ESTACION']).reindex(index_completo).reset_index()\n",
    "\n",
    "# Verificar fechas faltantes (para exportar listado)\n",
    "faltantes = df_plata[df_plata.isnull().any(axis=1)][['ESTACION', 'FECHA']]\n",
    "\n",
    "if not faltantes.empty:\n",
    "    faltantes.to_csv(PLATA_DIR / \"fechas_faltantes.txt\", index=False, sep='\\t')\n",
    "    print(\"Fechas faltantes exportadas a:\", PLATA_DIR / \"fechas_faltantes.txt\")\n",
    "else:\n",
    "    print(\"No se encontraron fechas faltantes\")\n",
    "\n",
    "# Mostrar ejemplo si hay faltantes\n",
    "print(faltantes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b85704-5591-470e-a46e-a43346980116",
   "metadata": {},
   "source": [
    "Esta estrategia asegura que cada estación tenga una fila para cada fecha del rango, incluso si originalmente no había registros ese día. Esto deja los valores faltantes como `NaN`, que luego se tratarán."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c913c6-b13f-416a-8d06-ce3e48fc53dc",
   "metadata": {},
   "source": [
    "## Tratamiento de valores nulos\n",
    "\n",
    "Luego de verificar fechas faltantes, analizamos los valores `NaN` dentro del dataset actual para decidir estrategias de imputación o tratamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204f65cc-3a3a-4170-908f-3902830d2b18",
   "metadata": {},
   "source": [
    "### Tratamiento de datos faltantes en el dataset diario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bfbac3-51b4-4b6b-8ceb-920aead48862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar cantidad de nulos por columna\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(df_plata.isnull().sum())\n",
    "\n",
    "# Calcular porcentaje de nulos por columna\n",
    "porcentaje_nulos = df_plata.isnull().mean() * 100\n",
    "print(\"\\nPorcentaje de valores nulos:\")\n",
    "print(porcentaje_nulos.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a8a240-a6f6-4609-b13c-0ea3aea43a1c",
   "metadata": {},
   "source": [
    "Una vez identificadas las columnas afectadas, proponemos distintas estrategias para completar los datos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e8c214-e0d0-4a2a-b7f6-6905872c9356",
   "metadata": {},
   "source": [
    "### Relleno con forward fill por estación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b407236-2e12-4451-9c17-492b77e80fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar por estación y fecha para aplicar forward fill correctamente\n",
    "df_plata_ffill = df_plata.sort_values(['ESTACION', 'FECHA']).copy()\n",
    "df_plata_ffill.update(df_plata.groupby('ESTACION').ffill())\n",
    "\n",
    "# Vista previa de ejemplo tras forward fill\n",
    "print(\"\\nEjemplo de datos tras forward fill:\")\n",
    "print(df_plata_ffill.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacaf95f-a649-4971-b4ee-9c042fe1db11",
   "metadata": {},
   "source": [
    "### Imputación con la media de cada estación (solo para columnas numéricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6159b92c-f4f2-4554-8400-2d043628374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputar con la media por estación\n",
    "columnas_a_imputar = ['TEMP_MEAN', 'PNM_MEAN', 'HUM_MEAN', 'WIND_SPEED_MEAN', 'WIND_DIR_MEAN']\n",
    "\n",
    "for col in columnas_a_imputar:\n",
    "    df_plata_ffill[col] = df_plata_ffill.groupby('ESTACION')[col].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# Verificar resultado tras imputación\n",
    "print(\"\\nValores nulos después de imputación con medias:\")\n",
    "print(df_plata_ffill[columnas_a_imputar].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3908a838-a9c0-4c8f-b1f5-b6005cfa8706",
   "metadata": {},
   "source": [
    "Estas estrategias permiten garantizar que las variables derivadas a construir se basen en datos consistentes, sin afectar la distribución ni introducir sesgos evidentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e25e56-6de6-47c7-9d84-967dd5acbdd0",
   "metadata": {},
   "source": [
    "### Tratamiento de datos faltantes en el dataset horario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbd7603-b5fb-446f-9f57-f12b36817782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar horarios reales de cada estación\n",
    "df_horario['HORA'] = df_horario['FECHA_HORA'].dt.hour\n",
    "horarios_por_estacion = df_horario.groupby('NOMBRE')['HORA'].value_counts().unstack(fill_value=0)\n",
    "horarios_mas_frecuentes = horarios_por_estacion.idxmax(axis=1)\n",
    "\n",
    "# Detectar horarios outlier (menos del 5% de los días)\n",
    "outliers_horarios = {}\n",
    "for estacion in horarios_por_estacion.index:\n",
    "    total_dias = df_horario[df_horario['NOMBRE'] == estacion]['FECHA_HORA'].dt.date.nunique()\n",
    "    outliers = horarios_por_estacion.loc[estacion][\n",
    "        horarios_por_estacion.loc[estacion] / total_dias < 0.05\n",
    "    ].index.tolist()\n",
    "    if outliers:\n",
    "        outliers_horarios[estacion] = outliers\n",
    "\n",
    "# Crear index completo por estación y sus horarios típicos\n",
    "df_horario['FECHA'] = df_horario['FECHA_HORA'].dt.floor('D')\n",
    "estaciones_h = df_horario['NOMBRE'].unique()\n",
    "fecha_h_min = df_horario['FECHA'].min()\n",
    "fecha_h_max = df_horario['FECHA'].max()\n",
    "rango_fechas = pd.date_range(start=fecha_h_min, end=fecha_h_max, freq='D')\n",
    "\n",
    "# Crear combinaciones válidas por estación\n",
    "porcentaje_frecuencia = 0.05 # al menos en 5% de los días\n",
    "\n",
    "index_completo_personalizado = []\n",
    "for estacion in estaciones_h:\n",
    "    total_dias_estacion = df_horario[df_horario['NOMBRE'] == estacion]['FECHA'].nunique()\n",
    "    horas_validas = horarios_por_estacion.columns[\n",
    "        (horarios_por_estacion.loc[estacion] / total_dias_estacion) >= porcentaje_frecuencia  \n",
    "    ].tolist()\n",
    "\n",
    "    for fecha in rango_fechas:\n",
    "        for hora in horas_validas:\n",
    "            index_completo_personalizado.append((estacion, pd.Timestamp(fecha + pd.Timedelta(hours=hora))))\n",
    "\n",
    "index_completo_h = pd.MultiIndex.from_tuples(index_completo_personalizado, names=['NOMBRE', 'FECHA_HORA'])\n",
    "\n",
    "# Reindexar para insertar valores faltantes en los horarios esperados únicamente\n",
    "df_horario_completo = df_horario.set_index(['NOMBRE', 'FECHA_HORA']).reindex(index_completo_h).reset_index()\n",
    "\n",
    "# Verificación\n",
    "print(\"\\nDiferencia de tamaño (horas originales vs completadas por horario habitual):\")\n",
    "print(\"Original:\", len(df_horario))\n",
    "print(\"Completo:\", len(df_horario_completo))\n",
    "print(\"\\nEjemplo de datos horarios con NaN insertados:\")\n",
    "print(df_horario_completo[df_horario_completo.isnull().any(axis=1)].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbe64ec-63a1-4df4-99b5-7b3a8f844f5f",
   "metadata": {},
   "source": [
    "### Mostrar horarios outliers detectados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa97cf-8e8b-4e6d-bf2c-5f77410e2855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar registros reales en horarios atípicos detectados\n",
    "print(\"\\n Registros reales en horarios atípicos:\")\n",
    "for estacion, horas in outliers_horarios.items():\n",
    "    print(f\" - {estacion}: {horas}\")\n",
    "\n",
    "# Registrar los registros reales que ocurren en horarios atípicos\n",
    "df_outliers_registros = []\n",
    "for estacion, horas_outlier in outliers_horarios.items():\n",
    "    registros_outlier = df_horario[\n",
    "        (df_horario['NOMBRE'] == estacion) &\n",
    "        (df_horario['HORA'].isin(horas_outlier))\n",
    "    ]\n",
    "    if not registros_outlier.empty:\n",
    "        df_outliers_registros.append(registros_outlier)\n",
    "\n",
    "# Concatenar y exportar si hay registros\n",
    "if df_outliers_registros:\n",
    "    df_outliers_concat = pd.concat(df_outliers_registros)\n",
    "    archivo_outliers = PLATA_DIR / \"registros_horarios_atipicos.csv\"\n",
    "    df_outliers_concat.to_csv(archivo_outliers, index=False)\n",
    "    print(\"\\n Archivo exportado con registros reales en horarios atípicos:\")\n",
    "    print(archivo_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555ab9e1-38b0-422b-8ac8-fabee6b2cc05",
   "metadata": {},
   "source": [
    "## Exportar datasets intermedios (antes de procesar los NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e123bf3d-a7ba-4ed6-b638-889d892267bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar datasets intermedios (si se desea conservar)\n",
    "df_plata.to_csv(PLATA_DIR / \"misiones_plata_con_nan.csv\", index=False)\n",
    "df_plata_ffill.to_csv(PLATA_DIR / \"misiones_plata_ffill.csv\", index=False)\n",
    "df_horario_completo.to_csv(PLATA_DIR / \"misiones_horario_completo.csv\", index=False)\n",
    "\n",
    "print(\"Archivos generados correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387836d4-c0de-457a-b4ff-7db2548ab541",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_horario_completo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b914b3-5333-417a-b3d7-4c406e1ee6ff",
   "metadata": {},
   "source": [
    "## Imputación de datos faltantes basada en promedio entre días anterior y posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c55f63-780e-4528-9572-12334333c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables a imputar\n",
    "variables_objetivo = ['TEMP', 'HUM', 'PNM', 'DD', 'FF']\n",
    "\n",
    "df_interp = df_horario_completo.copy()\n",
    "\n",
    "# Asegurar FECHA y HORA correctas\n",
    "df_interp['FECHA'] = df_interp['FECHA_HORA'].dt.date\n",
    "df_interp['HORA'] = df_interp['FECHA_HORA'].dt.hour\n",
    "\n",
    "# Ordenar por estación, fecha y hora\n",
    "df_interp = df_interp.sort_values(by=['NOMBRE', 'FECHA', 'HORA'])\n",
    "\n",
    "# Función de imputación por promedio entre día anterior y posterior\n",
    "def imputar_valores(grupo):\n",
    "    grupo = grupo.copy()  # para evitar advertencias de SettingWithCopy\n",
    "    for var in variables_objetivo:\n",
    "        for idx, fila in grupo.iterrows():\n",
    "            if pd.isna(fila[var]):\n",
    "                hora = fila['HORA']\n",
    "                fecha = fila['FECHA']\n",
    "\n",
    "                # Buscar el valor del día anterior\n",
    "                val_ant = grupo[(grupo['HORA'] == hora) & (grupo['FECHA'] < fecha)][var].last_valid_index()\n",
    "                val_ant = grupo.at[val_ant, var] if val_ant is not None else None\n",
    "\n",
    "                # Buscar el valor del día posterior\n",
    "                val_post = grupo[(grupo['HORA'] == hora) & (grupo['FECHA'] > fecha)][var].first_valid_index()\n",
    "                val_post = grupo.at[val_post, var] if val_post is not None else None\n",
    "\n",
    "                # Asignar promedio o valor disponible\n",
    "                if val_ant is not None and val_post is not None:\n",
    "                    grupo.at[idx, var] = round((val_ant + val_post) / 2, 1)\n",
    "                elif val_ant is not None:\n",
    "                    grupo.at[idx, var] = val_ant\n",
    "                elif val_post is not None:\n",
    "                    grupo.at[idx, var] = val_post\n",
    "    return grupo\n",
    "\n",
    "# Aplicar por estación SIN include_groups\n",
    "df_interp = (\n",
    "    df_interp.groupby('NOMBRE', group_keys=False)\n",
    "    .apply(imputar_valores)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Redondear valores numéricos a 1 decimal\n",
    "for var in variables_objetivo:\n",
    "    df_interp[var] = df_interp[var].round(1)\n",
    "\n",
    "# Ajustar tipos de columnas\n",
    "df_interp['HORA'] = df_interp['HORA'].astype('int64')\n",
    "if 'estacion_archivo' in df_interp.columns:\n",
    "    df_interp['estacion_archivo'] = df_interp['estacion_archivo'].astype('int64', errors='ignore')\n",
    "\n",
    "# Exportar\n",
    "archivo_imputado = PLATA_DIR / \"misiones_horario_imputado.csv\"\n",
    "df_interp.to_csv(archivo_imputado, index=False)\n",
    "print(f\"Archivo exportado: {archivo_imputado}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5a1e2-540a-4687-a19e-2fbad388c299",
   "metadata": {},
   "source": [
    "## Verificar las imputaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6171ed-7840-45f6-bebd-8b074f8307f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificación de imputación final\n",
    "print(\"Valores restantes faltantes por variable:\")\n",
    "print(df_interp[variables_objetivo].isnull().sum())\n",
    "\n",
    "# Vista previa de algunos valores aún faltantes (si existen)\n",
    "print(\"\\nEjemplos de filas con valores aún faltantes:\")\n",
    "print(df_interp[df_interp[variables_objetivo].isnull().any(axis=1)].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b35aff0-0b2a-4654-b75a-2da41925b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar imputaciones por columna\n",
    "imputaciones = {}\n",
    "for var in variables_objetivo:\n",
    "    # Detectar índices donde original es NaN pero imputado tiene valor\n",
    "    mask_imputado = df_horario_completo[var].isna() & df_interp[var].notna()\n",
    "    imputaciones[var] = mask_imputado.sum()\n",
    "\n",
    "# Mostrar resumen\n",
    "print(\"Resumen de imputaciones por variable:\")\n",
    "for var, count in imputaciones.items():\n",
    "    print(f\" - {var}: {count} valores imputados\")\n",
    "\n",
    "# Mostrar ejemplos comparativos (solo filas donde hubo imputación)\n",
    "print(\"\\nEjemplos de imputaciones realizadas:\")\n",
    "for var in variables_objetivo:\n",
    "    mask = df_horario_completo[var].isna() & df_interp[var].notna()\n",
    "    if mask.any():\n",
    "        print(f\"\\nVariable: {var}\")\n",
    "        print(df_interp.loc[mask, ['FECHA_HORA', 'NOMBRE', var]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fde00ba-b535-4101-aa38-7764982715d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carpeta con los archivos de días faltantes\n",
    "FALTANTES_DIR = Path(\"../data/faltantes\")  # Ajustar al directorio correcto\n",
    "variables_objetivo = ['TEMP', 'HUM', 'PNM', 'DD', 'FF']\n",
    "\n",
    "# Función para leer días faltantes de un archivo\n",
    "def leer_dias_faltantes(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    fechas = [line.strip() for line in lines if line.strip() and line.strip()[0].isdigit()]\n",
    "    return pd.to_datetime(fechas).date\n",
    "\n",
    "# Recorrer todos los archivos .txt de días faltantes\n",
    "faltantes_files = list(FALTANTES_DIR.glob(\"*.txt\"))\n",
    "\n",
    "resumen_resultados = []\n",
    "\n",
    "for file_path in faltantes_files:\n",
    "    estacion = file_path.stem.replace(\"dias_faltantes_\", \"\").replace(\"_\", \" \").upper()\n",
    "    dias_faltantes = leer_dias_faltantes(file_path)\n",
    "    \n",
    "    print(f\"\\n=== Estación: {estacion} ===\")\n",
    "    resultados_estacion = {\"Estación\": estacion, \"Total días faltantes\": len(dias_faltantes), \"Días completos\": 0, \"Días con NaN\": 0}\n",
    "    \n",
    "    for fecha in dias_faltantes:\n",
    "        subset_original = df_horario_completo[(df_horario_completo['NOMBRE'].str.upper() == estacion) & (df_horario_completo['FECHA'] == fecha)]\n",
    "        subset_imputado = df_interp[(df_interp['NOMBRE'].str.upper() == estacion) & (df_interp['FECHA'] == fecha)]\n",
    "        \n",
    "        if subset_imputado[variables_objetivo].isnull().any().any():\n",
    "            resultados_estacion[\"Días con NaN\"] += 1\n",
    "            print(f\"\\n⚠️ Fecha {fecha} aún con NaN\")\n",
    "        else:\n",
    "            resultados_estacion[\"Días completos\"] += 1\n",
    "            print(f\"\\n✅ Fecha {fecha} imputada correctamente\")\n",
    "        \n",
    "        # Comparar antes y después\n",
    "        if not subset_imputado.empty:\n",
    "            print(\"\\n--- Antes (Original con NaN) ---\")\n",
    "            print(subset_original[['FECHA_HORA', 'NOMBRE'] + variables_objetivo])\n",
    "            print(\"\\n--- Después (Imputado) ---\")\n",
    "            print(subset_imputado[['FECHA_HORA', 'NOMBRE'] + variables_objetivo])\n",
    "    \n",
    "    resumen_resultados.append(resultados_estacion)\n",
    "\n",
    "# Mostrar resumen final\n",
    "df_resumen = pd.DataFrame(resumen_resultados)\n",
    "print(\"\\nResumen de verificación de imputaciones por estación:\")\n",
    "print(df_resumen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c7ad1-8fc5-4237-bb4f-73822d928246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame reducido solo con días únicos por estación\n",
    "df_fechas = df_interp[['FECHA', 'NOMBRE']].drop_duplicates()\n",
    "\n",
    "# Generar rango completo de fechas\n",
    "fechas_totales = pd.date_range(start=df_fechas['FECHA'].min(), end=df_fechas['FECHA'].max(), freq='D')\n",
    "\n",
    "# Estaciones\n",
    "estaciones = df_fechas['NOMBRE'].unique()\n",
    "\n",
    "# Crear todas las combinaciones posibles (fecha, estación)\n",
    "index_completo = pd.MultiIndex.from_product([fechas_totales, estaciones], names=['FECHA', 'NOMBRE'])\n",
    "\n",
    "# Reindexar\n",
    "df_check = df_fechas.set_index(['FECHA', 'NOMBRE']).reindex(index_completo).reset_index()\n",
    "\n",
    "# Verificar faltantes\n",
    "faltantes = df_check[df_check.isnull().any(axis=1)][['NOMBRE', 'FECHA']]\n",
    "\n",
    "# Exportar resultados\n",
    "if not faltantes.empty:\n",
    "    archivo_faltantes_final = PLATA_DIR / \"fechas_faltantes_post_imputacion.txt\"\n",
    "    faltantes.to_csv(archivo_faltantes_final, index=False, sep='\\t')\n",
    "    print(f\"⚠️ Fechas faltantes exportadas a: {archivo_faltantes_final}\")\n",
    "else:\n",
    "    print(\"✅ No se encontraron fechas faltantes después de la imputación.\")\n",
    "\n",
    "# Vista rápida\n",
    "print(\"\\nEjemplo de fechas faltantes:\")\n",
    "print(faltantes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a409ae47-5b64-4a67-a84c-b5ef2cf82ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
