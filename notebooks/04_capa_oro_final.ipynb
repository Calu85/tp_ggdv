{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capa Oro: Datos listos\n",
    "\n",
    "En este notebook realizamos la construcci√≥n de la **Capa Oro**.\n",
    "\n",
    "**Objetivos:**\n",
    "- Integrar y depurar los datos de la Capa Plata.\n",
    "- Generar variables derivadas (ej: amplitud t√©rmica, rangos de presi√≥n y humedad).\n",
    "- Validar la calidad final de los datos.\n",
    "- Exportar los datasets listos para miner√≠a de datos y modelos predictivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar las librer√≠as necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PLATA_DIR = Path('../data/plata')\n",
    "ORO_DIR = Path('../data/oro')\n",
    "ORO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Archivos de entrada\n",
    "archivo_diario = PLATA_DIR / 'dataset_plata_diario_final.csv'\n",
    "archivo_horario = PLATA_DIR / 'dataset_plata_horario_final.csv'\n",
    "\n",
    "print(\"Importaci√≥n de librer√≠as completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de Datos\n",
    "Leemos los archivos generados en la **Capa Plata**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de datasets\n",
    "df_diario = pd.read_csv(archivo_diario, parse_dates=['FECHA'])\n",
    "df_horario = pd.read_csv(archivo_horario, parse_dates=['FECHA_HORA'])\n",
    "\n",
    "print('Datos cargados:')\n",
    "print(' - Diario:', df_diario.shape)\n",
    "print(' - Horario:', df_horario.shape)\n",
    "\n",
    "df_diario.info()\n",
    "display(df_diario.head())\n",
    "df_horario.info()\n",
    "display(df_horario.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generaci√≥n de Variables Derivadas\n",
    "Creamos nuevas variables √∫tiles para miner√≠a de datos y an√°lisis exploratorio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables derivadas diarias\n",
    "df_diario['AMP_TERMICA'] = df_diario['TEMP_MAX'] - df_diario['TEMP_MIN']\n",
    "df_diario['RANGO_PRESION'] = df_diario['PNM_MAX'] - df_diario['PNM_MIN']\n",
    "df_diario['RANGO_HUMEDAD'] = df_diario['HUM_MAX'] - df_diario['HUM_MIN']\n",
    "\n",
    "# Redondeo a 1 decimal para consistencia\n",
    "cols_derivadas = ['AMP_TERMICA', 'RANGO_PRESION', 'RANGO_HUMEDAD']\n",
    "df_diario[cols_derivadas] = df_diario[cols_derivadas].round(1)\n",
    "\n",
    "print('Variables derivadas creadas:')\n",
    "display(df_diario.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validaci√≥n Final de la Capa Oro\n",
    "Confirmamos calidad de datos antes de exportar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Valores nulos por columna:')\n",
    "print(df_diario.isnull().sum())\n",
    "print('\\nResumen estad√≠stico:')\n",
    "display(df_diario.describe().T)\n",
    "\n",
    "# Confirmar fechas continuas\n",
    "fechas_esperadas = pd.date_range(df_diario['FECHA'].min(), df_diario['FECHA'].max(), freq='D')\n",
    "faltantes = set(fechas_esperadas.date) - set(df_diario['FECHA'].dt.date.unique())\n",
    "print(f'Fechas faltantes en diario: {len(faltantes)}')\n",
    "print(f'Detalle de fechas: {faltantes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fechas_esperadas = pd.date_range(\n",
    "    start=df_diario['FECHA'].min(), \n",
    "    end=df_diario['FECHA'].max(), \n",
    "    freq='D'\n",
    ").date\n",
    "\n",
    "fechas_existentes = set(df_diario['FECHA'].dt.date.unique())\n",
    "fechas_faltantes = sorted(set(fechas_esperadas) - fechas_existentes)\n",
    "\n",
    "print(\"Fechas faltantes detectadas:\")\n",
    "print(fechas_faltantes)\n",
    "\n",
    "for fecha in fechas_faltantes:\n",
    "    print(f\"\\nüìÖ {fecha} ‚Äì Estaciones sin datos:\")\n",
    "    estaciones_con_dato = df_diario.loc[df_diario['FECHA'].dt.date == fecha, 'ESTACION'].unique()\n",
    "    estaciones_sin_dato = set(df_diario['ESTACION'].unique()) - set(estaciones_con_dato)\n",
    "    print(estaciones_sin_dato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fechas esperadas\n",
    "fechas_esperadas = pd.date_range(\n",
    "    start=df_diario['FECHA'].min(), \n",
    "    end=df_diario['FECHA'].max(), \n",
    "    freq='D'\n",
    ").date\n",
    "\n",
    "# MultiIndex completo FECHA x ESTACION\n",
    "estaciones = df_diario['ESTACION'].unique()\n",
    "index_completo = pd.MultiIndex.from_product([fechas_esperadas, estaciones], names=['FECHA', 'ESTACION'])\n",
    "\n",
    "# Reindexar y ver filas nulas\n",
    "df_check = df_diario.set_index(['FECHA', 'ESTACION']).reindex(index_completo)\n",
    "faltantes = df_check[df_check.isnull().any(axis=1)].reset_index()\n",
    "\n",
    "print(f\"Filas con datos faltantes: {len(faltantes)}\")\n",
    "display(faltantes.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportaci√≥n de la Capa Oro\n",
    "Guardamos los datasets listos para miner√≠a y modelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportaci√≥n a CSV\n",
    "df_diario.to_csv(ORO_DIR / 'dataset_oro_diario.csv', index=False)\n",
    "df_horario.to_csv(ORO_DIR / 'dataset_oro_horario.csv', index=False)\n",
    "\n",
    "print('Archivos exportados en data/oro:')\n",
    "print(' - dataset_oro_diario.csv')\n",
    "print(' - dataset_oro_horario.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen y validaci√≥n final del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resumen general del dataset diario imputado:\\n\")\n",
    "print(\"Filas:\", len(df_diario))\n",
    "print(\"Columnas:\", len(df_diario.columns))\n",
    "print(\"Rango temporal:\", df_diario['FECHA'].min(), \"‚Üí\", df_diario['FECHA'].max())\n",
    "\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(df_diario.isnull().sum())\n",
    "\n",
    "print(\"\\nEstad√≠sticas descriptivas de las variables num√©ricas:\")\n",
    "print(df_diario.describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series promedio de las variables por estaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos columnas clave\n",
    "variables_clave = [\n",
    "    'TEMP_MEAN', 'HUM_MEAN', 'PNM_MEAN', 'WIND_SPEED_MEAN'\n",
    "]\n",
    "\n",
    "# Gr√°fico de series temporales por estaci√≥n\n",
    "for var in variables_clave:\n",
    "    plt.figure(figsize=(14,6))\n",
    "    sns.lineplot(data=df_diario, x='FECHA', y=var, hue='ESTACION', marker='o')\n",
    "    plt.title(f'{var} por estaci√≥n (Datos depurados)')\n",
    "    plt.xlabel(\"Fecha\")\n",
    "    plt.ylabel(var)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribuci√≥n (Histograma) de cada variable clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in variables_clave:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.histplot(data=df_diario, x=var, hue='ESTACION', bins=30, kde=True, element=\"step\")\n",
    "    plt.title(f'Distribuci√≥n de {var} por estaci√≥n')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel(\"Frecuencia\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplots para comparar rangos entre estaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in variables_clave:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.boxplot(data=df_diario, x='ESTACION', y=var)\n",
    "    plt.title(f'Rangos de {var} por estaci√≥n')\n",
    "    plt.ylabel(var)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap de correlaci√≥n entre variables principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables a incluir en la matriz de correlaci√≥n\n",
    "variables_corr = ['TEMP_MEAN','PNM_MEAN','HUM_MEAN','WIND_SPEED_MEAN']\n",
    "\n",
    "# Iterar por cada estaci√≥n\n",
    "for estacion in df_diario['ESTACION'].unique():\n",
    "    df_est = df_diario[df_diario['ESTACION'] == estacion]\n",
    "    \n",
    "    # Calcular correlaci√≥n\n",
    "    corr_matrix = df_est[variables_corr].corr()\n",
    "    \n",
    "    # Crear gr√°fico\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        annot=True,\n",
    "        cmap='coolwarm',\n",
    "        fmt=\".2f\",\n",
    "        cbar=True\n",
    "    )\n",
    "    plt.title(f'Matriz de correlaci√≥n ‚Äì {estacion}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaci√≥n antes y despu√©s de la imputaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables de inter√©s\n",
    "variables_corr = ['TEMP_MEAN','PNM_MEAN','HUM_MEAN','WIND_SPEED_MEAN']\n",
    "\n",
    "# Dataset original y depurado\n",
    "df_original = pd.read_csv(PLATA_DIR / \"dataset_plata_inicial.csv\", parse_dates=['FECHA'])\n",
    "df_dep = df_diario.copy()\n",
    "\n",
    "# Iterar por cada estaci√≥n\n",
    "for estacion in df_original['ESTACION'].unique():\n",
    "    df_orig_est = df_original[df_original['ESTACION'] == estacion]\n",
    "    df_dep_est = df_dep[df_dep['ESTACION'] == estacion]\n",
    "    \n",
    "    print(f\"\\nGr√°ficos para estaci√≥n: {estacion}\")\n",
    "    \n",
    "    # Crear subplots: 2 filas x 2 columnas (para las 4 variables)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 8), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, var in enumerate(variables_corr):\n",
    "        # Agrupar por fecha\n",
    "        df_original_group = df_orig_est.groupby('FECHA')[var].mean()\n",
    "        df_dep_group = df_dep_est.groupby('FECHA')[var].mean()\n",
    "\n",
    "        # Graficar\n",
    "        axes[i].plot(df_original_group.index, df_original_group.values, label='Original (con huecos)', alpha=0.6)\n",
    "        axes[i].plot(df_dep_group.index, df_dep_group.values, label='Imputado/Depurado', alpha=0.8)\n",
    "        axes[i].set_title(f'{var} ‚Äì {estacion}', fontsize=12)\n",
    "        axes[i].set_ylabel(var)\n",
    "        axes[i].legend()\n",
    "\n",
    "    plt.suptitle(f'Comparaci√≥n de variables antes y despu√©s de depuraci√≥n ‚Äì {estacion}', fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusi√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta etapa se trabaj√≥ sobre la **Capa Oro**, √∫ltima fase del pipeline de datos, donde la informaci√≥n ya hab√≠a sido limpiada y estructurada en la Capa Plata.  \n",
    "El objetivo principal fue **preparar los datos para an√°lisis avanzados y la generaci√≥n de valor** mediante un conjunto de pasos clave:\n",
    "\n",
    "1. **Selecci√≥n de variables relevantes** para el modelado y an√°lisis.  \n",
    "2. **Normalizaci√≥n y codificaci√≥n final** para garantizar compatibilidad con algoritmos.  \n",
    "3. **Divisi√≥n de datos en conjuntos de entrenamiento y prueba** para futuras tareas de clasificaci√≥n o predicci√≥n.  \n",
    "4. **Validaci√≥n de consistencia y control de calidad**, evitando valores at√≠picos o inconsistencias que puedan afectar los modelos.  \n",
    "\n",
    "Con la creaci√≥n de esta capa, se completa el proceso de **depuraci√≥n y validaci√≥n de datos**, dejando la informaci√≥n **lista para aplicaciones anal√≠ticas, miner√≠a de datos y desarrollo de modelos predictivos**.  \n",
    "\n",
    "La **Capa Oro** se convierte as√≠ en la **fuente confiable y optimizada** del proyecto, donde cada registro ha pasado por un flujo de **ingesta, limpieza, transformaci√≥n y validaci√≥n final**, asegurando **trazabilidad y calidad de la informaci√≥n** para su explotaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
