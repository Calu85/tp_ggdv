{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0303ce6d",
   "metadata": {},
   "source": [
    "# Ingesta y Capa Bronce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938a3d2d",
   "metadata": {},
   "source": [
    "En esta notebook se inicia la construcción del pipeline de datos meteorológicos, trabajando con los archivos crudos provistos por el SMN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c87778",
   "metadata": {},
   "source": [
    "## Importar las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573fb540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Importación de librerías completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47201f0b",
   "metadata": {},
   "source": [
    "## Configuración de paths y carpetas del proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path('..').resolve()\n",
    "RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "BRONCE_DIR = BASE_DIR / 'data' / 'bronce'\n",
    "\n",
    "# Crear carpetas si no existen\n",
    "for path in [BRONCE_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Iniciación de carpetas del proyecto completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cba45b",
   "metadata": {},
   "source": [
    "## Lectura del archivo de estaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed669ec-bf6e-4b2c-b021-07ec4589fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del archivo\n",
    "archivo_estaciones = RAW_DIR / 'estaciones' / 'estaciones_smn.txt'\n",
    "\n",
    "# Leer todas las líneas, omitiendo las dos primeras (encabezado y unidades)\n",
    "with open(archivo_estaciones, \"r\", encoding=\"latin1\") as f:\n",
    "    lines = f.readlines()[2:]\n",
    "\n",
    "# Expresión regular para extraer campos:\n",
    "# Modifico la expresión porque tenía algunos problemas:\n",
    "# 1. Una de las estaciones de jujuy no tiene código OACI y la expresión ignoraba la linea. Para resolverlo hago la extracción del código oaci opcional.\n",
    "# 2. Hay dos estaciones con nombres largos y entonces la separación entre nombre y provincia es menor a 2 caracteres. Para resolverlo, como el archivo esta organizado en columnas fijas, uso la columna 31 como límite entre nombre y provincia.\n",
    "# Con estos cambios, ahora sí la cantidad de estaciones se carga correctamente (118) y las provincias se cuentan como 25 (las 23 + CABA y antártida), en lugar de 26 provincias y 117 estaciones de antes.\n",
    "\n",
    "pattern = re.compile(\n",
    "    r\"^(?P<nombre>.{31})(?P<provincia>.+?)\\s{2,}(?P<lat_gr>-?\\d+)\\s+(?P<lat_min>\\d+)\\s+(?P<lon_gr>-?\\d+)\\s+(?P<lon_min>\\d+)\\s+(?P<altura_m>\\d+)\\s+(?P<numero>\\d+)(?:\\s+(?P<numero_oaci>\\S+))?\\s*$\"\n",
    ")\n",
    "\n",
    "# Extraer los datos\n",
    "data = []\n",
    "for line in lines:\n",
    "    match = pattern.match(line)\n",
    "    if match:\n",
    "        data.append(match.groupdict())\n",
    "\n",
    "# Crear DataFrame\n",
    "df_estaciones = pd.DataFrame(data)\n",
    "\n",
    "# Conversión de tipos\n",
    "df_estaciones[['lat_gr', 'lat_min', 'lon_gr', 'lon_min', 'altura_m', 'numero']] = df_estaciones[[\n",
    "    'lat_gr', 'lat_min', 'lon_gr', 'lon_min', 'altura_m', 'numero'\n",
    "]].apply(pd.to_numeric)\n",
    "\n",
    "# Cargar las provincias\n",
    "provincias_unicas = df_estaciones['provincia'].str.strip().str.upper().unique()\n",
    "\n",
    "#En los archivos de datos horarios el nombre de algunas estaciones está truncado en 26 caracteres y los demás pasan a la linea siguiente.\n",
    "#Esto hace que esas escaciones queden sin detectar ya que se realiza una comparación de strings que no son iguales. Justo córdoba (la provincia que elegí) tiene dos estaciones de nombres largos\n",
    "# que quedan sin detectar. Para resolver esto trunco los nombres de las estaciones a 26 caracteres. No tiene efectos adversos porque esa cantidad de caracteres es suficiente para seguir teniendo nombres únicos en cada estación.\n",
    "\n",
    "df_estaciones['nombre'] = df_estaciones['nombre'].str.slice(0, 26)\n",
    "\n",
    "# Imprimir la cantidad de estaciones registradas\n",
    "print(\"Estaciones cargadas:\", len(df_estaciones))\n",
    "\n",
    "# Imprimir la cantidad de provincias registradas\n",
    "print(\"Cantidad de provincias:\", len(provincias_unicas))\n",
    "\n",
    "# Imprimir las provincias\n",
    "print(\"Provincias disponibles:\", provincias_unicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e0212-b23e-4dc8-b400-e328d513cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estaciones.iloc[45:55]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0893f232-07a4-4f22-87d3-83c45da4a556",
   "metadata": {},
   "source": [
    "# Trato de geolocalizar las estaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa322e9-282d-4062-b348-f168ad716a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dms_to_decimal(deg, minutes):\n",
    "    sign = 1 if deg >= 0 else -1\n",
    "    return sign * (abs(deg) + minutes / 60)\n",
    "\n",
    "df_estaciones[\"LAT_DEC\"] = df_estaciones.apply(lambda row: dms_to_decimal(row[\"lat_gr\"], row[\"lat_min\"]), axis=1)\n",
    "df_estaciones[\"LON_DEC\"] = df_estaciones.apply(lambda row: dms_to_decimal(row[\"lon_gr\"], row[\"lon_min\"]), axis=1)\n",
    "df_estaciones[[\"LAT_DEC\", \"LON_DEC\"]].to_csv(BRONCE_DIR / 'coordenadas_estaciones.csv', index=False)\n",
    "df_estaciones.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a9de26",
   "metadata": {},
   "source": [
    "## Selección de estaciones. \n",
    "\n",
    "### Para el desarrollo del trabajo se utilizarán las estaciones ubicadas en la provincia de Córdoba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e73a4-ae19-4a13-9709-93f131a03792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingresar el nombre de la provincia con la que se va a trabajar\n",
    "provincia = 'CORDOBA'\n",
    "\n",
    "df_provincia = df_estaciones[df_estaciones['provincia'].str.upper() == provincia]\n",
    "df_provincia[['nombre', 'provincia', 'numero', 'numero_oaci']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a92af-1e2b-4851-8929-6e6a8476500c",
   "metadata": {},
   "source": [
    "## Filtrar las estaciones que correspondan a la provincia seleccionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac45e6ae-4c6a-422b-80e2-3125b1a5f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se selecciona una fecha para visualizar los datos\n",
    "archivo_dato = RAW_DIR / 'datohorario' /  '_procesados' / 'datohorario20250531.txt'\n",
    "\n",
    "# Leer todas las líneas, omitiendo las dos primeras (encabezado y unidades)\n",
    "with open(archivo_dato, \"r\", encoding=\"latin1\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Detectar columnas separadas por múltiples espacios\n",
    "columnas = re.split(r\"\\s{2,}\", lines[0].strip())\n",
    "\n",
    "# Leer datos\n",
    "data = [\n",
    "    re.split(r\"\\s{2,}\", line.strip(), maxsplit=len(columnas)-1)\n",
    "    for line in lines[1:]\n",
    "    if len(line.strip()) > 0 and not line.isspace()\n",
    "]\n",
    "\n",
    "# Crear DataFrame con columnas originales\n",
    "df_dato = pd.DataFrame(data, columns=columnas)\n",
    "df_dato.columns = df_dato.columns.str.strip()\n",
    "df_dato[\"NOMBRE\"] = df_dato[\"NOMBRE\"].str.strip()\n",
    "\n",
    "# Filtrar por estaciones\n",
    "nombres_provincia = df_provincia[\"nombre\"].str.strip().unique()\n",
    "df_provincia_dia = df_dato[df_dato[\"NOMBRE\"].isin(nombres_provincia)]\n",
    "\n",
    "# Crear copia y convertir tipos SOLO para impresión de tipos correctos\n",
    "df_tipos = df_provincia_dia.copy()\n",
    "df_tipos[\"FECHA\"] = pd.to_datetime(df_tipos[\"FECHA\"], format=\"%d%m%Y\", errors=\"coerce\").dt.date\n",
    "df_tipos[\"HORA\"] = pd.to_numeric(df_tipos[\"HORA\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df_tipos[\"TEMP\"] = pd.to_numeric(df_tipos[\"TEMP\"], errors=\"coerce\")\n",
    "df_tipos[\"HUM\"] = pd.to_numeric(df_tipos[\"HUM\"], errors=\"coerce\")\n",
    "df_tipos[\"PNM\"] = pd.to_numeric(df_tipos[\"PNM\"], errors=\"coerce\")\n",
    "df_tipos[\"DD\"] = pd.to_numeric(df_tipos[\"DD\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df_tipos[\"FF\"] = pd.to_numeric(df_tipos[\"FF\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Mostrar todos los resultados\n",
    "print(df_provincia_dia.to_string(index=False))\n",
    "print()\n",
    "print(\"Columnas:\", df_dato.columns.tolist())\n",
    "print(\"Tipos de dato:\")\n",
    "print(df_tipos.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3832851a",
   "metadata": {},
   "source": [
    "## Procesamiento por estación y por fecha (con limpieza y reporte resumen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af8f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear carpeta de salida si no existe\n",
    "BRONCE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Buscar todos los archivos datohorario disponibles\n",
    "archivos_datos = sorted(glob(str(RAW_DIR / \"datohorario\" / '_procesados' / \"datohorario*.txt\")))\n",
    "\n",
    "errores_globales = 0\n",
    "\n",
    "for archivo in archivos_datos:\n",
    "    try:\n",
    "        with open(archivo, encoding=\"latin1\") as f:\n",
    "            raw_lines = f.readlines()\n",
    "\n",
    "        header = raw_lines[0].strip()\n",
    "        columnas = re.split(r\"\\s{2,}\", header)\n",
    "\n",
    "        data = [\n",
    "            re.split(r\"\\s{2,}\", line.strip(), maxsplit=len(columnas)-1)\n",
    "            for line in raw_lines[1:]\n",
    "            if len(line.strip()) > 0 and not line.isspace()\n",
    "        ]\n",
    "\n",
    "        df_dato = pd.DataFrame(data, columns=columnas)\n",
    "        df_dato.columns = df_dato.columns.str.strip()\n",
    "        df_dato[\"NOMBRE\"] = df_dato[\"NOMBRE\"].str.strip()\n",
    "\n",
    "        # Filtrar por estaciones según la provincia\n",
    "        df_provincia = df_dato[df_dato[\"NOMBRE\"].isin(nombres_provincia)]\n",
    "\n",
    "        # Obtener fecha\n",
    "        fecha = Path(archivo).stem.replace(\"datohorario\", \"\")\n",
    "\n",
    "        # Guardar archivos por estación\n",
    "        for nombre in nombres_provincia:\n",
    "            nombre_clean = nombre.lower().replace(\" \", \"_\")\n",
    "            df_estacion = df_provincia[df_provincia[\"NOMBRE\"] == nombre]\n",
    "\n",
    "            if not df_estacion.empty:\n",
    "                path_estacion = BRONCE_DIR / nombre_clean\n",
    "                path_estacion.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # Archivos de salida\n",
    "                archivo_csv = path_estacion / f\"{fecha}.csv\"\n",
    "                df_estacion.to_csv(archivo_csv, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        errores_globales += 1\n",
    "        continue\n",
    "\n",
    "# Reporte final\n",
    "print(\"Proceso completado.\")\n",
    "print(f\"Días procesados: {len(archivos_datos)}\")\n",
    "print(f\"Errores al procesar archivos: {errores_globales}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f9c0bc-09c9-4e51-9ef8-e975403b2422",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "\n",
    "En este notebook realizamos el proceso de **ingesta de datos meteorológicos** y la creación de la **Capa Bronce** de nuestro proyecto:\n",
    "\n",
    "1. **Lectura de datos crudos** provenientes de archivos del Servicio Meteorológico Nacional.\n",
    "2. **Estructuración inicial de datos**, manteniendo la información tal como llega del entorno real, sin limpieza ni transformación.\n",
    "3. **Organización en la estructura de carpetas** del proyecto, asegurando que los datos queden almacenados en la capa correspondiente para futuras etapas de procesamiento.\n",
    "\n",
    "Esta etapa constituye la **base del pipeline de datos**, preservando la trazabilidad y sirviendo como fuente de verdad para las capas posteriores (Plata y Oro)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
