{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0303ce6d",
   "metadata": {},
   "source": [
    "# Clase 3 – Ingesta y Capa Bronce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938a3d2d",
   "metadata": {},
   "source": [
    "En esta notebook se inicia la construcción del pipeline de datos meteorológicos, trabajando con los archivos crudos provistos por el SMN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c87778",
   "metadata": {},
   "source": [
    "## Importar las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573fb540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47201f0b",
   "metadata": {},
   "source": [
    "## Configuración de paths y carpetas del proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path('..').resolve()\n",
    "RAW_DIR = BASE_DIR / 'data' / 'raw'\n",
    "BRONCE_DIR = BASE_DIR / 'data' / 'bronce'\n",
    "\n",
    "# Crear carpetas si no existen\n",
    "for path in [BRONCE_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cba45b",
   "metadata": {},
   "source": [
    "## Lectura del archivo de estaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del archivo\n",
    "archivo_estaciones = RAW_DIR / 'estaciones' / 'estaciones_smn.txt'\n",
    "\n",
    "# Leer todas las líneas, omitiendo las dos primeras (encabezado y unidades)\n",
    "with open(archivo_estaciones, \"r\", encoding=\"latin1\") as f:\n",
    "    lines = f.readlines()[2:]\n",
    "\n",
    "# Expresión regular para extraer campos:\n",
    "pattern = re.compile(\n",
    "    r\"^(?P<nombre>.+?)\\s{2,}(?P<provincia>.+?)\\s{2,}(?P<lat_gr>-?\\d+)\\s+(?P<lat_min>\\d+)\\s+(?P<lon_gr>-?\\d+)\\s+(?P<lon_min>\\d+)\\s+(?P<altura_m>\\d+)\\s+(?P<numero>\\d+)\\s+(?P<numero_oaci>\\S+)\\s*$\"\n",
    ")\n",
    "\n",
    "# Extraer los datos\n",
    "data = []\n",
    "for line in lines:\n",
    "    match = pattern.match(line)\n",
    "    if match:\n",
    "        data.append(match.groupdict())\n",
    "\n",
    "# Crear DataFrame\n",
    "df_estaciones = pd.DataFrame(data)\n",
    "\n",
    "# Conversión de tipos\n",
    "df_estaciones[['lat_gr', 'lat_min', 'lon_gr', 'lon_min', 'altura_m', 'numero']] = df_estaciones[[\n",
    "    'lat_gr', 'lat_min', 'lon_gr', 'lon_min', 'altura_m', 'numero'\n",
    "]].apply(pd.to_numeric)\n",
    "\n",
    "# Cargar las provincias\n",
    "provincias_unicas = df_estaciones['provincia'].str.strip().str.upper().unique()\n",
    "\n",
    "# Imprimir la cantidad de estaciones registradas\n",
    "print(\"Estaciones cargadas:\", len(df_estaciones))\n",
    "\n",
    "# Imprimir la cantidad de provincias registradas\n",
    "print(\"Cantidad de provincias:\", len(provincias_unicas))\n",
    "\n",
    "# Imprimir las provincias\n",
    "print(\"Provincias disponibles:\", provincias_unicas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a9de26",
   "metadata": {},
   "source": [
    "## Selección de estaciones. \n",
    "\n",
    "### Para el desarrollo del trabajo se utilizarán las estaciones ubicadas en la provincia de Misiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355e73a4-ae19-4a13-9709-93f131a03792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingresar el nombre de la provincia con la que se va a trabajar\n",
    "provincia = 'MISIONES'\n",
    "\n",
    "df_provincia = df_estaciones[df_estaciones['provincia'].str.upper() == provincia]\n",
    "df_provincia[['nombre', 'provincia', 'numero', 'numero_oaci']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a92af-1e2b-4851-8929-6e6a8476500c",
   "metadata": {},
   "source": [
    "## Filtrar las estaciones que correspondan a la provincia seleccionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac45e6ae-4c6a-422b-80e2-3125b1a5f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se selecciona una fecha para visualizar los datos\n",
    "archivo_dato = RAW_DIR / 'datohorario' / 'datohorario20250531.txt'\n",
    "\n",
    "# Leer todas las líneas, omitiendo las dos primeras (encabezado y unidades)\n",
    "with open(archivo_dato, \"r\", encoding=\"latin1\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Detectar columnas separadas por múltiples espacios\n",
    "columnas = re.split(r\"\\s{2,}\", lines[0].strip())\n",
    "\n",
    "# Leer datos\n",
    "data = [\n",
    "    re.split(r\"\\s{2,}\", line.strip(), maxsplit=len(columnas)-1)\n",
    "    for line in lines[1:]\n",
    "    if len(line.strip()) > 0 and not line.isspace()\n",
    "]\n",
    "\n",
    "# Crear DataFrame con columnas originales\n",
    "df_dato = pd.DataFrame(data, columns=columnas)\n",
    "df_dato.columns = df_dato.columns.str.strip()\n",
    "df_dato[\"NOMBRE\"] = df_dato[\"NOMBRE\"].str.strip()\n",
    "\n",
    "# Filtrar por estaciones\n",
    "nombres_provincia = df_provincia[\"nombre\"].str.strip().unique()\n",
    "df_provincia_dia = df_dato[df_dato[\"NOMBRE\"].isin(nombres_provincia)]\n",
    "\n",
    "# Crear copia y convertir tipos SOLO para impresión de tipos correctos\n",
    "df_tipos = df_provincia_dia.copy()\n",
    "df_tipos[\"FECHA\"] = pd.to_datetime(df_tipos[\"FECHA\"], format=\"%d%m%Y\", errors=\"coerce\").dt.date\n",
    "df_tipos[\"HORA\"] = pd.to_numeric(df_tipos[\"HORA\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df_tipos[\"TEMP\"] = pd.to_numeric(df_tipos[\"TEMP\"], errors=\"coerce\")\n",
    "df_tipos[\"HUM\"] = pd.to_numeric(df_tipos[\"HUM\"], errors=\"coerce\")\n",
    "df_tipos[\"PNM\"] = pd.to_numeric(df_tipos[\"PNM\"], errors=\"coerce\")\n",
    "df_tipos[\"DD\"] = pd.to_numeric(df_tipos[\"DD\"], errors=\"coerce\").astype(\"Int64\")\n",
    "df_tipos[\"FF\"] = pd.to_numeric(df_tipos[\"FF\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# Mostrar todos los resultados\n",
    "print(df_provincia_dia.to_string(index=False))\n",
    "print()\n",
    "print(\"Columnas:\", df_dato.columns.tolist())\n",
    "print(\"Tipos de dato:\")\n",
    "print(df_tipos.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3832851a",
   "metadata": {},
   "source": [
    "## Procesamiento por estación y por fecha (con limpieza y reporte resumen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af8f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear carpeta de salida si no existe\n",
    "BRONCE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Buscar todos los archivos datohorario disponibles\n",
    "archivos_datos = sorted(glob(str(RAW_DIR / \"datohorario\" / \"datohorario*.txt\")))\n",
    "\n",
    "errores_globales = 0\n",
    "\n",
    "for archivo in archivos_datos:\n",
    "    try:\n",
    "        with open(archivo, encoding=\"latin1\") as f:\n",
    "            raw_lines = f.readlines()\n",
    "\n",
    "        header = raw_lines[0].strip()\n",
    "        columnas = re.split(r\"\\s{2,}\", header)\n",
    "\n",
    "        data = [\n",
    "            re.split(r\"\\s{2,}\", line.strip(), maxsplit=len(columnas)-1)\n",
    "            for line in raw_lines[1:]\n",
    "            if len(line.strip()) > 0 and not line.isspace()\n",
    "        ]\n",
    "\n",
    "        df_dato = pd.DataFrame(data, columns=columnas)\n",
    "        df_dato.columns = df_dato.columns.str.strip()\n",
    "        df_dato[\"NOMBRE\"] = df_dato[\"NOMBRE\"].str.strip()\n",
    "\n",
    "        # Filtrar por estaciones según la provincia\n",
    "        df_provincia = df_dato[df_dato[\"NOMBRE\"].isin(nombres_provincia)]\n",
    "\n",
    "        # Obtener fecha\n",
    "        fecha = Path(archivo).stem.replace(\"datohorario\", \"\")\n",
    "\n",
    "        # Guardar archivos por estación\n",
    "        for nombre in nombres_provincia:\n",
    "            nombre_clean = nombre.lower().replace(\" \", \"_\")\n",
    "            df_estacion = df_provincia[df_provincia[\"NOMBRE\"] == nombre]\n",
    "\n",
    "            if not df_estacion.empty:\n",
    "                path_estacion = BRONCE_DIR / nombre_clean\n",
    "                path_estacion.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # Archivos de salida\n",
    "                archivo_parquet = path_estacion / f\"{fecha}.parquet\"\n",
    "                archivo_csv = path_estacion / f\"{fecha}.csv\"\n",
    "                archivo_txt = path_estacion / f\"{fecha}.txt\"\n",
    "                \n",
    "                # Guardar en distintos formatos\n",
    "                df_estacion.to_parquet(archivo_parquet, index=False)\n",
    "                df_estacion.to_csv(archivo_csv, index=False)\n",
    "\n",
    "                with open(archivo_txt, \"w\", encoding=\"latin1\") as f:\n",
    "                    f.write(\"  \".join(df_estacion.columns) + \"\\n\")\n",
    "                    for _, row in df_estacion.iterrows():\n",
    "                        f.write(\"  \".join(map(str, row.values)) + \"\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        errores_globales += 1\n",
    "        continue\n",
    "\n",
    "# Reporte final\n",
    "print(\"✅ Proceso completado.\")\n",
    "print(f\"Días procesados: {len(archivos_datos)}\")\n",
    "print(f\"Errores al procesar archivos: {errores_globales}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
