{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a61d964",
   "metadata": {},
   "source": [
    "# Exploración de datos y preparación (Capa Plata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8065f283-72b8-44aa-8280-2b0793b03857",
   "metadata": {},
   "source": [
    "En esta clase trabajaremos con los datos meteorológicos previamente filtrados por estación (Capa Bronce) para realizar una primera exploración estructurada. El objetivo es transformar los datos crudos en un conjunto limpio, validado y enriquecido, listo para su análisis.\n",
    "\n",
    "## Objetivos de la clase:\n",
    "- Identificar tipos de análisis exploratorio.\n",
    "- Aplicar transformaciones básicas en la Capa Plata.\n",
    "- Detectar y tratar valores nulos y atípicos.\n",
    "- Generar estadísticas descriptivas por estación y fecha.\n",
    "- Visualizar las principales variables meteorológicas.\n",
    "- Exportar el dataset en múltiples formatos para su posterior análisis.\n",
    "\n",
    "El foco estará puesto en la limpieza, validación, estandarización y normalización de datos para facilitar su consumo en dashboards, análisis y modelos posteriores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8256ad83",
   "metadata": {},
   "source": [
    "## Importación de librerías y configuración de paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import os\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from windrose import WindroseAxes\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Definir rutas para capas Bronce y Plata y Diccionario\n",
    "BRONCE_DIR = Path('../data/bronce/')\n",
    "PLATA_DIR = Path('../data/plata/')\n",
    "PLATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# Crear carpeta para faltantes si no existe\n",
    "FALTANTES_DIR = Path('../data/faltantes/')\n",
    "FALTANTES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# Crear carpeta para guardar los metadatos\n",
    "DICCIONARIO_DIR = Path('../data/diccionario/')\n",
    "DICCIONARIO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Variable de la provincia para generar los metadatos\n",
    "PROVINCIA = 'Misiones'\n",
    "\n",
    "# Ajustar el ancho máximo para impresión en consola\n",
    "pd.set_option('display.max_columns', None)  # Mostrar todas las columnas\n",
    "pd.set_option('display.width', 300)         # Ajustar a un ancho suficiente en consola\n",
    "pd.set_option('display.max_colwidth', None) # Evitar recortes en contenido de celdas\n",
    "\n",
    "print(\"Importación de librerías completada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863caef1",
   "metadata": {},
   "source": [
    "## Carga de archivos de todas las estaciones de la provincia seleccionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ff7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar todos los archivos CSV de la capa Bronce (filtrados por estaciones)\n",
    "archivos = list(BRONCE_DIR.rglob(\"*.csv\"))\n",
    "dfs = []\n",
    "\n",
    "for archivo in archivos:\n",
    "    df = pd.read_csv(archivo)\n",
    "    df['estacion_archivo'] = archivo.stem  # Agregar nombre del archivo como identificador de estación\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_estaciones = pd.concat(dfs, ignore_index=True)\n",
    "print(df_estaciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2624f8c9-d827-4a6c-9913-24c4dbaf3f3b",
   "metadata": {},
   "source": [
    "## Normalización y combinación de fecha y hora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f57b8c9-7b45-40bd-9a35-77eb9dd91109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir FECHA (DDMMAAAA) a string y formatear como DDMMAAAA\n",
    "df_estaciones['FECHA'] = df_estaciones['FECHA'].astype(str).str.zfill(8)\n",
    "\n",
    "# Convertir a formato datetime (indicando el formato original)\n",
    "df_estaciones['FECHA'] = pd.to_datetime(df_estaciones['FECHA'], format='%d%m%Y', errors='coerce')\n",
    "\n",
    "# Asegurar que HORA está en número entero (algunos datasets los tienen como string)\n",
    "df_estaciones['HORA'] = df_estaciones['HORA'].astype(int)\n",
    "\n",
    "# Crear columna combinada FECHA_HORA como datetime completo\n",
    "df_estaciones['FECHA_HORA'] = df_estaciones['FECHA'] + pd.to_timedelta(df_estaciones['HORA'], unit='h')\n",
    "\n",
    "# Verificamos el resultado\n",
    "print(df_estaciones[['FECHA', 'HORA', 'FECHA_HORA']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9898fef1-6cb5-45a4-80e7-81dc88bac3bb",
   "metadata": {},
   "source": [
    "## Guardar el archivo con los datos horarios de las estaciones de la provincia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4f8d6-241a-4051-8bcd-9b54ebc96f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_horario = PLATA_DIR / \"horario_archivo.csv\"\n",
    "\n",
    "# Guardar el archivo con datos horarios\n",
    "df_estaciones.to_csv(archivo_horario, index=False)\n",
    "\n",
    "print(f\"Archivo horario exportado correctamente a:\\n{archivo_horario}\")\n",
    "print(f\"Filas: {len(df_estaciones)} – Columnas: {len(df_estaciones.columns)}\")\n",
    "print(\"\\nVista previa de datos horarios:\")\n",
    "print(df_estaciones.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e744eff5-6cde-45bc-bb75-4127194aecd0",
   "metadata": {},
   "source": [
    "### Agrupación diaria de variables por estación\n",
    "\n",
    "En esta sección agrupamos los datos meteorológicos por estación y por fecha. \n",
    "Calculamos estadísticas clave como temperatura, presión, humedad, dirección y velocidad del viento. \n",
    "Esto permite obtener un resumen diario limpio para cada estación, facilitando su análisis posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f320826-bf84-4874-98e9-9753fd2064d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por estación y fecha para obtener valores resumen diarios\n",
    "df_estaciones_group = df_estaciones.groupby(['NOMBRE', 'FECHA']).agg({\n",
    "    'TEMP': ['mean', 'min', 'max'],\n",
    "    'PNM': ['mean', 'min', 'max'],\n",
    "    'HUM': ['mean', 'min', 'max'],\n",
    "    'DD': 'mean',\n",
    "    'FF': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Renombrar columnas para facilitar su uso\n",
    "df_estaciones_group.columns = ['ESTACION', 'FECHA',\n",
    "                               'TEMP_MEAN', 'TEMP_MIN', 'TEMP_MAX',\n",
    "                               'PNM_MEAN', 'PNM_MIN', 'PNM_MAX',\n",
    "                               'HUM_MEAN', 'HUM_MIN', 'HUM_MAX',\n",
    "                               'WIND_DIR_MEAN', 'WIND_SPEED_MEAN']\n",
    "\n",
    "print(df_estaciones_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05024bce-60cc-4c68-9215-db052e99f05b",
   "metadata": {},
   "source": [
    "## Validación estructural: detección de duplicados\n",
    "\n",
    "Antes de avanzar con el análisis, verificamos si existen filas duplicadas por combinación de estación y fecha, lo cual indicaría problemas en la agregación de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ade6bae-34ef-4c14-94b8-a0be009ad085",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicados = df_estaciones_group.duplicated(subset=['ESTACION', 'FECHA']).sum()\n",
    "print(f\" Duplicados por estación y fecha: {duplicados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4c05b-242d-45f3-af9b-ef3882dc2b32",
   "metadata": {},
   "source": [
    "## Análisis de cobertura temporal por estación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de296092-3728-4a57-a340-c4d71a6fd6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear rango completo de fechas\n",
    "rango_fechas = pd.date_range(df_estaciones_group['FECHA'].min(), df_estaciones_group['FECHA'].max())\n",
    "\n",
    "# Detectar días faltantes por estación y exportar\n",
    "for estacion in df_estaciones_group['ESTACION'].unique():\n",
    "    fechas_est = pd.to_datetime(df_estaciones_group[df_estaciones_group['ESTACION'] == estacion]['FECHA'].unique())\n",
    "    dias_faltantes = sorted(set(rango_fechas) - set(fechas_est))\n",
    "\n",
    "    # Crear archivo por estación\n",
    "    nombre_archivo = FALTANTES_DIR / f'dias_faltantes_{estacion.replace(\" \", \"_\").lower()}.txt'\n",
    "    with open(nombre_archivo, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Días faltantes para estación: {estacion}\\n\")\n",
    "        f.write(f\"Total: {len(dias_faltantes)}\\n\\n\")\n",
    "        for dia in dias_faltantes:\n",
    "            f.write(f\"{dia.strftime('%Y-%m-%d')}\\n\")\n",
    "\n",
    "    # Mostrar resumen por consola\n",
    "    print(f\" Estación: {estacion}\")\n",
    "    print(f\" Días faltantes: {len(dias_faltantes)}\")\n",
    "    print(f\" Archivo: {nombre_archivo.name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f2293c-32af-4441-9636-f27c7a859216",
   "metadata": {},
   "source": [
    "## Exploración univariada de variables meteorológicas\n",
    "\n",
    "A continuación, se presentan histogramas para explorar la distribución de variables meteorológicas clave. Esto permite identificar posibles sesgos, valores extremos o asimetrías en la distribución de cada variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa120573-e103-4a82-a748-53b573c929c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables a graficar\n",
    "variables_hist = {\n",
    "    'TEMP_MEAN': 'Temperatura promedio diaria (°C)',\n",
    "    'HUM_MEAN': 'Humedad relativa promedio diaria (%)',\n",
    "    'PNM_MEAN': 'Presión atmosférica promedio diaria (hPa)',\n",
    "    'WIND_SPEED_MEAN': 'Velocidad promedio del viento (km/h)'\n",
    "}\n",
    "\n",
    "# Crear subplots para los histogramas\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, (var, titulo) in enumerate(variables_hist.items(), 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.histplot(df_estaciones_group[var], kde=True, bins=30, color='skyblue')\n",
    "    plt.title(titulo, fontsize=12)\n",
    "    plt.xlabel(titulo)\n",
    "    plt.ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Crear gráfico Windrose para WIND_DIR_MEAN\n",
    "df_viento = df_estaciones[(df_estaciones['DD'] <= 360)]\n",
    "\n",
    "ax = WindroseAxes.from_ax()\n",
    "ax.bar(df_viento['DD'], df_viento['FF'], normed=True, opening=0.8, edgecolor='white')\n",
    "\n",
    "# Ajustes de visualización\n",
    "ax.set_title(\"Distribución direccional del viento\", fontsize=13)\n",
    "ax.set_legend(loc='lower right', title=\"Frecuencia (%)\", fontsize=9, title_fontsize=10, frameon=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285026bf",
   "metadata": {},
   "source": [
    "## Análisis exploratorio – valores máximos, mínimos, promedio diario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores inválidos en DD (mayores a 360)\n",
    "valores_dd_invalidos = df_estaciones[df_estaciones['DD'] > 360]['DD'].unique()\n",
    "print(\"Valores inválidos en DD (mayores a 360):\", valores_dd_invalidos, \"\\n\")\n",
    "\n",
    "# Crear columna de fecha sin hora para agrupar\n",
    "df_estaciones['FECHA_DIA'] = df_estaciones['FECHA'].dt.date\n",
    "\n",
    "# Crear DD_VALID con NaN en valores > 360 (para excluir del promedio)\n",
    "df_estaciones['DD_VALID'] = df_estaciones['DD'].where(df_estaciones['DD'] <= 360, pd.NA)\n",
    "\n",
    "# Paso 3: Agrupar por estación y día, y calcular estadísticas\n",
    "df_estaciones_group = df_estaciones.groupby(['NOMBRE', 'FECHA_DIA']).agg({\n",
    "    'TEMP': ['mean', 'min', 'max'],\n",
    "    'PNM': ['mean', 'min', 'max'],\n",
    "    'HUM': ['mean', 'min', 'max'],\n",
    "    'DD_VALID': ['mean'],       # Promedio con valores válidos solamente\n",
    "    'DD': ['min', 'max'],       # Para conservar min y max sin filtrar\n",
    "    'FF': ['mean', 'min', 'max']\n",
    "}).reset_index()\n",
    "\n",
    "# Renombrar columnas para facilitar lectura\n",
    "df_estaciones_group.columns = [\n",
    "    'ESTACION', 'FECHA',\n",
    "    'TEMP_MEAN', 'TEMP_MIN', 'TEMP_MAX',\n",
    "    'PNM_MEAN', 'PNM_MIN', 'PNM_MAX',\n",
    "    'HUM_MEAN', 'HUM_MIN', 'HUM_MAX',\n",
    "    'WIND_DIR_MEAN',  # <- Esta es la media con valores válidos\n",
    "    'WIND_DIR_MIN', 'WIND_DIR_MAX',\n",
    "    'WIND_SPEED_MEAN', 'WIND_SPEED_MIN', 'WIND_SPEED_MAX'\n",
    "]\n",
    "\n",
    "# Redondear solo las columnas *_MEAN a 1 decimal\n",
    "cols_mean = ['TEMP_MEAN', 'PNM_MEAN', 'HUM_MEAN', 'WIND_DIR_MEAN', 'WIND_SPEED_MEAN']\n",
    "df_estaciones_group[cols_mean] = df_estaciones_group[cols_mean].round(1)\n",
    "\n",
    "# Vista previa\n",
    "print(df_estaciones_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6571e2cc-4e29-4aab-9b8d-568eb54fb26b",
   "metadata": {},
   "source": [
    "## Visualización por estación y por variable\n",
    "\n",
    "Se generan gráficos individuales para cada estación meteorológica, mostrando la evolución diaria de temperatura, presión, humedad y viento. \n",
    "También se incluye una visualización tipo Windrose para observar la distribución de la dirección y velocidad del viento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55458f9-ecc2-4952-a6c7-e75927e6a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar que FECHA esté en datetime\n",
    "df_estaciones_group['FECHA'] = pd.to_datetime(df_estaciones_group['FECHA'])\n",
    "\n",
    "variables = {\n",
    "    'TEMP': {'min': 'TEMP_MIN', 'mean': 'TEMP_MEAN', 'max': 'TEMP_MAX', 'label': 'Temperatura (°C)'},\n",
    "    'HUM': {'min': 'HUM_MIN', 'mean': 'HUM_MEAN', 'max': 'HUM_MAX', 'label': 'Humedad (%)'},\n",
    "    'PNM': {'min': 'PNM_MIN', 'mean': 'PNM_MEAN', 'max': 'PNM_MAX', 'label': 'Presión (hPa)'},\n",
    "    'WIND_SPEED': {'min': 'WIND_SPEED_MIN', 'mean': 'WIND_SPEED_MEAN', 'max': 'WIND_SPEED_MAX', 'label': 'Velocidad del viento (km/h)'}\n",
    "}\n",
    "\n",
    "# Recorrer estaciones\n",
    "for estacion in df_estaciones_group['ESTACION'].unique():\n",
    "    df_est_group = df_estaciones_group[df_estaciones_group['ESTACION'] == estacion]\n",
    "    df_est_raw = df_estaciones[(df_estaciones['NOMBRE'] == estacion) & (df_estaciones['DD'] <= 360)]\n",
    "\n",
    "    print(f\"\\n Estación: {estacion}\")\n",
    "\n",
    "    # Una fila por variable (cada una con 3 columnas: min, mean, max)\n",
    "    for var, datos in variables.items():\n",
    "        fig = make_subplots(\n",
    "            rows=1, cols=3,\n",
    "            shared_yaxes=True,\n",
    "            subplot_titles=(\"Mínimo\", \"Máximo\", \"Promedio\")\n",
    "        )\n",
    "\n",
    "        colores = {\n",
    "            'min': 'blue',\n",
    "            'max': 'red',\n",
    "            'mean': 'green'\n",
    "        }\n",
    "        \n",
    "        for i, tipo in enumerate(['min', 'max', 'mean']):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_est_group['FECHA'],\n",
    "                    y=df_est_group[datos[tipo]],\n",
    "                    mode='lines+markers',\n",
    "                    name=f\"{datos['label']} – {tipo}\",\n",
    "                    line=dict(color=colores[tipo]),\n",
    "                    hovertemplate=\"Fecha: %{x|%d-%m-%Y}<br>Valor: %{y:.2f}<extra></extra>\"\n",
    "                ),\n",
    "                row=1, col=i + 1\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text=f\"{datos['label']} – {estacion}\",\n",
    "            height=400,\n",
    "            showlegend=False\n",
    "        )\n",
    "        fig.update_xaxes(tickangle=45)\n",
    "        fig.show()\n",
    "\n",
    "    # Graficar rosa del viento con matplotlib (se mantiene igual)\n",
    "    ax = WindroseAxes.from_ax()\n",
    "    ax.bar(df_est_raw['DD'], df_est_raw['FF'], normed=True, opening=0.8, edgecolor='white')\n",
    "    ax.set_title(f'Dirección y velocidad del viento – {estacion}', fontsize=12)\n",
    "    ax.set_legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2c552a-5154-4e32-8fa9-7507eb684df7",
   "metadata": {},
   "source": [
    "## Normalización Min-Max: ¿Por qué la aplicamos?\n",
    "\n",
    "Cuando trabajamos con múltiples variables meteorológicas —como temperatura, presión, humedad, dirección y velocidad del viento— nos enfrentamos a un problema: **cada una está en una escala diferente**.\n",
    "\n",
    "- Temperatura: 5 °C a 35 °C  \n",
    "- Presión atmosférica: 1000 a 1030 hPa  \n",
    "- Humedad relativa: 40 % a 100 %  \n",
    "- Dirección del viento: 0° a 360°  \n",
    "- Velocidad del viento: 0 a 30 km/h  \n",
    "\n",
    "Estas diferencias pueden dificultar tanto la comparación visual como el análisis conjunto. Para resolverlo, aplicamos la **normalización Min-Max**, que transforma cada valor al rango [0, 1] manteniendo su proporcionalidad, mediante la siguiente fórmula:\n",
    "\n",
    "$$\n",
    "x_{\\text{norm}} = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}\n",
    "$$\n",
    "\n",
    "Donde:  \n",
    "- $x$ es el valor original  \n",
    "- $x_{\\text{min}}$ es el mínimo observado de la variable  \n",
    "- $x_{\\text{max}}$ es el máximo observado de la variable  \n",
    "\n",
    "### ¿Qué logramos con esto?\n",
    "\n",
    "- Facilitar la **comparación visual** entre variables heterogéneas  \n",
    "- Preparar los datos para **análisis multivariado** (como clustering o reducción de dimensiones)  \n",
    "- Evitar que variables con mayor escala **dominen** sobre otras en modelos o gráficos  \n",
    "- Posibilitar visualizaciones integradas, como **gráficos radar o líneas normalizadas**\n",
    "\n",
    "> Esta transformación no modifica la forma de la distribución, sino únicamente su escala, lo que permite trabajar todas las variables en igualdad de condiciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ef1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar cantidad de valores nulos antes del rellenado\n",
    "print(\"🔍 Valores nulos por columna antes del forward fill:\\n\")\n",
    "print(df_estaciones_group.isnull().sum())\n",
    "\n",
    "# Rellenar valores nulos con forward fill\n",
    "df_estaciones_group.ffill(inplace=True)\n",
    "\n",
    "# Definir columnas *_MEAN para normalizar\n",
    "cols_mean = ['TEMP_MEAN', 'PNM_MEAN', 'HUM_MEAN', 'WIND_DIR_MEAN', 'WIND_SPEED_MEAN']\n",
    "\n",
    "# Aplicar normalización Min-Max y mostrar ejemplos\n",
    "for col in cols_mean:\n",
    "    min_val = df_estaciones_group[col].min()\n",
    "    max_val = df_estaciones_group[col].max()\n",
    "    df_estaciones_group[col + '_NORM'] = (df_estaciones_group[col] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Imprimir resumen\n",
    "    print(f\"\\n🔹 Normalización de {col}:\")\n",
    "    print(f\"   Valor mínimo: {min_val:.2f}, máximo: {max_val:.2f}\")\n",
    "    print(df_estaciones_group[[col, col + '_NORM']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19408960",
   "metadata": {},
   "source": [
    "## Exportación a Capa Plata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27fc2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar la Capa Plata:\n",
    "df_estaciones_group.to_csv(PLATA_DIR / 'dataset_plata_inicial.csv', index=False)\n",
    "\n",
    "# Imprimir resumen de exportación\n",
    "print(\"\\n Archivos exportados en formato Capa Plata:\")\n",
    "print(f\" - CSV:     {PLATA_DIR / 'dataset_plata_inicial.csv'}\")\n",
    "print(f\"\\n Filas exportadas: {len(df_estaciones_group)}\")\n",
    "print(f\" Columnas exportadas: {len(df_estaciones_group.columns)}\")\n",
    "print(f\"\\n Vista previa:\")\n",
    "print(df_estaciones_group.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04ed9e3-20f8-4c55-a01c-9d7651fb9af3",
   "metadata": {},
   "source": [
    "## Generación de metadatos y documentación del dataset\n",
    "\n",
    "Para garantizar la trazabilidad y comprensión del dataset procesado, se generan tres archivos complementarios:\n",
    "\n",
    "- **Metadatos de variables**: resumen estadístico por columna (tipo, nulos, valores únicos, mínimos, máximos, etc.).\n",
    "- **Diccionario de variables**: descripción manual del significado, unidad y observaciones de cada campo.\n",
    "- **Metadatos generales**: información sobre la fuente, cobertura geográfica y temporal, formato, y responsable del procesamiento.\n",
    "\n",
    "Estos archivos permiten documentar correctamente la Capa Plata, facilitando su interpretación y reutilización en etapas posteriores del pipeline de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b44574-5fea-4a88-860f-257b6360f107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Función para generar metadatos por columna ---\n",
    "def generar_metadatos(df):\n",
    "    metadatos = []\n",
    "    for col in df.columns:\n",
    "        serie = df[col]\n",
    "        tipo = serie.dtype\n",
    "        no_nulos = serie.notnull().sum()\n",
    "        nulos = serie.isnull().sum()\n",
    "        pct_nulos = (nulos / len(serie)) * 100\n",
    "        unicos = serie.nunique()\n",
    "        ejemplo = serie.dropna().iloc[0] if no_nulos > 0 else None\n",
    "        try:\n",
    "            minimo = serie.min()\n",
    "            maximo = serie.max()\n",
    "        except:\n",
    "            minimo, maximo = None, None\n",
    "\n",
    "        metadatos.append({\n",
    "            'Columna': col,\n",
    "            'Tipo de dato': str(tipo),\n",
    "            'Valores no nulos': no_nulos,\n",
    "            'Valores nulos': nulos,\n",
    "            '% Nulos': round(pct_nulos, 2),\n",
    "            'Valor mínimo': minimo,\n",
    "            'Valor máximo': maximo,\n",
    "            'Valores únicos': unicos,\n",
    "            'Ejemplo': ejemplo\n",
    "        })\n",
    "    return pd.DataFrame(metadatos)\n",
    "\n",
    "# --- Diccionario de variables (manual) ---\n",
    "diccionario_vars = pd.DataFrame([\n",
    "    [\"ESTACION\", \"Nombre de la estación meteorológica\", \"-\", \"Agrupación principal\"],\n",
    "    [\"FECHA\", \"Fecha de la medición\", \"AAAA-MM-DD\", \"Convertido desde DDMMAAAA\"],\n",
    "    [\"TEMP_MIN\", \"Temperatura mínima diaria\", \"°C\", \"\"],\n",
    "    [\"TEMP_MEAN\", \"Temperatura promedio diaria\", \"°C\", \"\"],\n",
    "    [\"TEMP_MAX\", \"Temperatura máxima diaria\", \"°C\", \"\"],\n",
    "    [\"PNM_MIN\", \"Presión mínima diaria\", \"hPa\", \"\"],\n",
    "    [\"PNM_MEAN\", \"Presión promedio diaria\", \"hPa\", \"\"],\n",
    "    [\"PNM_MAX\", \"Presión máxima diaria\", \"hPa\", \"\"],\n",
    "    [\"HUM_MIN\", \"Humedad relativa mínima diaria\", \"%\", \"\"],\n",
    "    [\"HUM_MEAN\", \"Humedad relativa promedio diaria\", \"%\", \"\"],\n",
    "    [\"HUM_MAX\", \"Humedad relativa máxima diaria\", \"%\", \"\"],\n",
    "    [\"WIND_DIR_MEAN\", \"Dirección promedio del viento\", \"°\", \"Se excluyeron valores > 360\"],\n",
    "    [\"WIND_SPEED_MEAN\", \"Velocidad promedio del viento\", \"km/h\", \"\"],\n",
    "    [\"TEMP_MEAN_NORM\", \"TEMP_MEAN normalizada Min-Max\", \"[0, 1]\", \"Para análisis multivariado\"],\n",
    "    [\"PNM_MEAN_NORM\", \"PNM_MEAN normalizada Min-Max\", \"[0, 1]\", \"\"],\n",
    "    [\"HUM_MEAN_NORM\", \"HUM_MEAN normalizada Min-Max\", \"[0, 1]\", \"\"],\n",
    "    [\"WIND_DIR_MEAN_NORM\", \"WIND_DIR_MEAN normalizada Min-Max\", \"[0, 1]\", \"\"],\n",
    "    [\"WIND_SPEED_MEAN_NORM\", \"WIND_SPEED_MEAN normalizada Min-Max\", \"[0, 1]\", \"\"]\n",
    "], columns=[\"Columna\", \"Descripción\", \"Unidad\", \"Observaciones\"])\n",
    "\n",
    "# --- Metadatos generales automáticos ---\n",
    "estaciones = df_estaciones['NOMBRE'].unique()\n",
    "provincia = estaciones[0].split()[-1].capitalize()\n",
    "cobertura_geo = f\"Estaciones meteorológicas de la provincia de {provincia}, Argentina\"\n",
    "\n",
    "fecha_min = df_estaciones['FECHA'].min().strftime('%Y-%m-%d')\n",
    "fecha_max = df_estaciones['FECHA'].max().strftime('%Y-%m-%d')\n",
    "cobertura_temporal = f\"Desde {fecha_min} hasta {fecha_max}\"\n",
    "\n",
    "metadatos_generales = pd.DataFrame([\n",
    "    [\"Nombre del conjunto de datos\", \"misiones_plata\"],\n",
    "    [\"Fuente original\", \"Servicio Meteorológico Nacional (SMN)\"],\n",
    "    [\"Cobertura geográfica\", cobertura_geo],\n",
    "    [\"Cobertura temporal\", cobertura_temporal],\n",
    "    [\"Frecuencia\", \"Datos horarios, agregados a diario\"],\n",
    "    [\"Unidad de observación\", \"Estación meteorológica\"],\n",
    "    [\"Formato de archivo\", \"CSV, Parquet, TXT (delimitado por tabulaciones)\"],\n",
    "    [\"Fecha de procesamiento\", date.today().isoformat()],\n",
    "    [\"Responsable del procesamiento\", \"Equipo de análisis de datos - FIUBA\"],\n",
    "    [\"Nivel del dataset\", \"Capa Plata (datos limpios, normalizados y listos para análisis)\"]\n",
    "], columns=[\"Campo\", \"Valor\"])\n",
    "\n",
    "# --- Exportar los tres archivos ---\n",
    "metadatos_df = generar_metadatos(df_estaciones_group)\n",
    "path_metadatos = DICCIONARIO_DIR / 'metadatos_variables.csv'\n",
    "path_diccionario = DICCIONARIO_DIR / 'diccionario_variables.csv'\n",
    "path_generales = DICCIONARIO_DIR / 'metadatos_generales.csv'\n",
    "\n",
    "metadatos_df.to_csv(path_metadatos, index=False)\n",
    "diccionario_vars.to_csv(path_diccionario, index=False)\n",
    "metadatos_generales.to_csv(path_generales, index=False)\n",
    "\n",
    "# --- Resumen por consola ---\n",
    "print(\"\\n Metadatos exportados:\")\n",
    "print(f\" - {path_metadatos.name} ({len(metadatos_df)} columnas analizadas)\")\n",
    "print(f\" - {path_generales.name} (resumen general del dataset)\")\n",
    "\n",
    "print(\"\\n Diccionario de variables exportado:\")\n",
    "print(f\" - {path_diccionario.name} ({len(diccionario_vars)} filas)\")\n",
    "\n",
    "print(\"\\n Vista previa de los archivos generados:\\n\")\n",
    "\n",
    "print(\"🔹 Metadatos por variable:\")\n",
    "print(pd.read_csv(path_metadatos).head(3), \"\\n\")\n",
    "\n",
    "print(\"🔹 Diccionario de variables:\")\n",
    "print(pd.read_csv(path_diccionario).head(3), \"\\n\")\n",
    "\n",
    "print(\"🔹 Metadatos generales:\")\n",
    "print(pd.read_csv(path_generales).head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d08d86-b3a1-44bb-bbfd-d4dad7c8f67c",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "\n",
    "Durante esta clase construimos la **Capa Plata** del pipeline de datos aplicando buenas prácticas de limpieza y exploración. Logramos:\n",
    "\n",
    "- Transformar fechas y crear una columna temporal (`FECHA_HORA`).\n",
    "- Calcular métricas diarias por estación para temperatura, presión, humedad y viento.\n",
    "- Filtrar valores inválidos (como direcciones de viento > 360).\n",
    "- Normalizar variables para facilitar comparaciones entre estaciones.\n",
    "- Visualizar el comportamiento de cada variable y la dirección predominante del viento.\n",
    "- Exportar el conjunto resultante en múltiples formatos (CSV, Parquet, TXT).\n",
    "\n",
    "Este dataset enriquecido será la base para los análisis posteriores en las próximas clases (Clases 5 a 7), incluyendo minería de datos, clasificación y visualización avanzada.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
